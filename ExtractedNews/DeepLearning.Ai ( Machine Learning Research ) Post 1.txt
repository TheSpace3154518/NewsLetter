https://www.deeplearning.ai/the-batch/tag/research/
(FID) (which measures similarity between generated and non-generated images, lower is better), while the unmodified version reached 19.5 FID.
Why it matters: Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other’s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.
Experiments with modified and unmodified versions of SiT-XL/2 yielded similar results.
Trained to convergence, the modified models outperformed the unmodified versions. For instance, the modified  SiT-XL/2 achieved 5.9 FID (after 4 million training steps), while the unmodified version achieved 8.3 FID (after 7 million training steps).
What’s new: Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposed Representation Alignment (REPA), a loss term for transformer-based diffusion.
We’re thinking: It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.
Given the embedding with noise added, the diffusion model learned to remove the noise according to the usual loss term.
Stay updated with weekly AI News and Insights delivered to your inbox
pretrained encoder to embed an image.
Picture describing Visual model aligning diffusion embeddings with DINOv2 encoders using REPA and DiT/SiT blocks.
Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.
Key insight: Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn’t need to learn how to embed an image from scratch.
In 400,000 training steps, the modified model reached 12.3
Pretrained Embeddings Accelerate Diffusion Transformers’ Learning
Pretrained embeddings accelerate diffusion transformers’ learning
The models continued to learn at different speeds as training continued. The modified DiT-XL/2  took 850,000 training steps to reach 9.6 FID, while the unmodified version took 7 million steps to reach the same number.
Results: The modified DiT-XL/2 learned significantly faster than the unmodified version.
Faster Learning for Diffusion Models Pretrained embeddings accelerate diffusion transformers’ learning
How it works: The authors modified DiT-XL/2 and SiT-XL/2 transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrained DINOv2.
At inference, given pure noise, the model removed it over several steps to produce an image embedding. Stable Diffusion VAE’s decoder converted the embedding into an image.
It also learned according to the REPA loss. Specifically, it learned to maximize the cosine similarity between a specially processed version of its eighth-layer embedding and the embedding produced by a pretrained DINOv2. To process its eighth-layer embedding for the REPA loss, the diffusion model fed the embedding to a vanilla neural network.