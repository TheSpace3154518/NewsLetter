https://openai.com/index/paperbench/
ChatGPT(opens in a new window)
Documentation(opens in a new window)
Help center(opens in a new window)
To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge’s performance by creating a separate benchmark for judges
Evaluating AI’s Ability to Replicate AI Research.
View code(opens in a new window)
API Log in(opens in a new window)
Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments
Sora(opens in a new window)
We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0%
For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria
our code to facilitate future research in understanding the AI engineering capabilities of AI agents.
Help Center(opens in a new window)
Sora log in(opens in a new window)
Read paper(opens in a new window)
We open-source⁠(opens in a new window) our code to facilitate future research in understanding the AI engineering capabilities of AI agents.
Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline
In total, PaperBench contains 8,316 individually gradable tasks
Developer Forum(opens in a new window)
open-source⁠(opens in a new window)
API Platform(opens in a new window)
We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research
Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism
API log in(opens in a new window)